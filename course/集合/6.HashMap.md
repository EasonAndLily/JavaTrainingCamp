# Map之HashMap

![Java-HashMap](https://tva1.sinaimg.cn/large/e6c9d24ely1go5v92ilnfj20xc0k00wo.jpg)

## [HashMap](https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html)

基于哈希表的Map接口的实现。此实现提供所有可选的映射操作，并允许空值和空键。 （HashMap类与Hashtable大致等效，不同之处在于它是不同步的，并且允许为null。）该类不保证映射的顺序；它不保证映射的顺序。特别是，它不能保证顺序会随着时间的推移保持恒定。

假设哈希函数将元素正确分散在存储桶中，则此实现为基本操作（获取和放置）提供恒定时间的性能。集合视图上的迭代所需的时间与HashMap实例的“容量”（存储桶数）及其大小（键-值映射数）成正比。因此，如果迭代性能很重要，则不要将初始容量设置得过高（或负载因子过低），这一点非常重要。

>An instance of HashMap has two parameters that affect its performance: initial capacity and load factor. The capacity is the number of buckets in the hash table, and the initial capacity is simply the capacity at the time the hash table is created. The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased. When the number of entries in the hash table exceeds the product of the load factor and the current capacity, the hash table is rehashed (that is, internal data structures are rebuilt) so that the hash table has approximately twice the number of buckets.

>As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, including get and put). The expected number of entries in the map and its load factor should be taken into account when setting its initial capacity, so as to minimize the number of rehash operations. If the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.

通常，默认负载因子（0.75）在时间和空间成本之间提供了一个很好的折衷方案。较高的值会减少空间开销，但会增加查找成本（在HashMap类的大多数操作中都得到体现，包括get和put）。设置其初始容量时，应考虑映射中的预期条目数及其负载因子，以最大程度地减少重新哈希操作的数量。如果初始容量大于最大条目数除以负载因子，则将不会进行任何哈希操作。

>If many mappings are to be stored in a HashMap instance, creating it with a sufficiently large capacity will allow the mappings to be stored more efficiently than letting it perform automatic rehashing as needed to grow the table. Note that using many keys with the same hashCode() is a sure way to slow down performance of any hash table. To ameliorate impact, when keys are Comparable, this class may use comparison order among keys to help break ties.

如果许多映射要存储在一个HashMap实例中，那么使用足够大的容量创建它将使映射能够更有效地存储，而不是根据需要执行自动重新散列来增长表。注意，使用多个键和相同的hashCode()肯定会降低任何哈希表的性能。为了减轻影响，当键具有可比性时，该类可以使用键之间的比较顺序来帮助打破联系。

**Note that this implementation is not synchronized.** If multiple threads access a hash map concurrently, and at least one of the threads modifies the map structurally, it must be synchronized externally. (A structural modification is any operation that adds or deletes one or more mappings; merely changing the value associated with a key that an instance already contains is not a structural modification.) This is typically accomplished by synchronizing on some object that naturally encapsulates the map. If no such object exists, the map should be "wrapped" using the Collections.synchronizedMap method. This is best done at creation time, to prevent accidental unsynchronized access to the map:`Map m = Collections.synchronizedMap(new HashMap(...));`

## 创建与遍历

### 构造函数

HashMap为我们提供了四种构造函数：

* HashMap()：Constructs an empty HashMap with the default initial capacity (16) and the default load factor (0.75).
* HashMap(int initialCapacity)：Constructs an empty HashMap with the specified initial capacity and the default load factor (0.75).
* HashMap(int initialCapacity, float loadFactor)：Constructs an empty HashMap with the specified initial capacity and load factor.
* HashMap(Map<? extends K,? extends V> m)：Constructs a new HashMap with the same mappings as the specified Map.

### 遍历

1. **通过KeySet集合来遍历**

```java
Map<Integer, String> map = new HashMap<>();
for (Integer key : map.keySet()) {
    System.out.println(key);
    System.out.println(map.get(key));
}
```

2. **通过Entry接口来遍历**

```java
Map<Integer, String> map = new HashMap<>();
for(Map.Entry<Integer, String> entry : map.entrySet()) {
    System.out.println(entry.getKey());
    System.out.println(entry.getValue());
}
```

3. **通过迭代器遍历**

```java
Map<Integer, String> map = new HashMap<>();
Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();
while (iterator.hasNext()) {
    Map.Entry<Integer, String> entry = iterator.next();
    System.out.println(entry.getKey());
    System.out.println(entry.getValue());
}
```

4. **通过Java8的forEach方法来遍历**

```java
Map<Integer, String> map = new HashMap<>();
map.forEach((key, value) -> {
    System.out.println(key);
    System.out.println(value);
});
```

### 四种遍历方法的性能对比

我们使用10000000条数据用于测试，测试代码如下：

```java
public class Main {
    public static void main(String[] args) throws InterruptedException {
        Map<Integer, String> map = new HashMap<>();
        for (int i = 0; i < 10000000; i++) {
            map.put(i, "value" + i);
        }
        keySet(map);
        entry(map);
        iterator(map);
        foreach(map);
    }

    private static void keySet(Map<Integer, String> map) {
        Instant startTime = Instant.now();
        for (Integer key : map.keySet()) {
            // test
        }
        System.out.println("keySet Speed Time: " + (Instant.now().getNano() - startTime.getNano()));
    }

    private static void entry(Map<Integer, String> map) {
        Instant startTime = Instant.now();
        for (Map.Entry<Integer, String> entry : map.entrySet()) {
            // test
        }
        System.out.println("Entry Speed Time: " + (Instant.now().getNano() - startTime.getNano()));
    }

    private static void iterator(Map<Integer, String> map) {
        Instant startTime = Instant.now();
        Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();
        while (iterator.hasNext()) {
            Map.Entry<Integer, String> entry = iterator.next();
        }
        System.out.println("Iterator Speed Time: " + (Instant.now().getNano() - startTime.getNano()));
    }

    private static void foreach(Map<Integer, String> map) {
        Instant startTime = Instant.now();
        map.forEach((key, value) -> {
            //test
        });
        System.out.println("Foreach Speed Time: " + (Instant.now().getNano() - startTime.getNano()));
    }
```

输出结果为：
```bash
keySet Speed Time: 155159000
Entry Speed Time: 118331000
Iterator Speed Time: 128656000
Foreach Speed Time: 96296000
```

多次运行，结果与上面的类似，这就说明，当数据量很大的情况下，`forEach`遍历最快，`keySet`遍历最慢，其他两种旗鼓相当。





